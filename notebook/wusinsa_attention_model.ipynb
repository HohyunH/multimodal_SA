{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wusinsa_attention_model.ipynb","provenance":[{"file_id":"1kv_e2l6Dbk6Cg2X8QTgo2kHYlnkWNGC8","timestamp":1636028543024},{"file_id":"1QHKMOuE_UEl5ONzY4izFDoTgXCWFvdv6","timestamp":1636012859623},{"file_id":"1YKznP0ZOiOMLNau7fPIkzzBuA4vimEnI","timestamp":1623401372622},{"file_id":"160Mt72UWmk6JYQzTivMBLLfwcIWSWeoa","timestamp":1620219634444}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"hO1WYGuO1SoQ"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4urHKWd0_Q7","executionInfo":{"status":"ok","timestamp":1636110253567,"user_tz":-540,"elapsed":23136,"user":{"displayName":"Hohyun Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18073395245965959785"}},"outputId":"672fefab-e80b-4c33-894e-259bb3532cc6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"K2rjTqMf-XUb"},"source":["## 라이브러리 다운로드"]},{"cell_type":"code","metadata":{"id":"lz61i8-YjOeF"},"source":["!pip install torchtext==0.4.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBvTSERNjiqK","executionInfo":{"status":"ok","timestamp":1636110285814,"user_tz":-540,"elapsed":28367,"user":{"displayName":"Hohyun Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18073395245965959785"}},"outputId":"ef1486a2-6670-4500-944a-a0b96f3de5d4"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from torchtext import data, datasets\n","from torchtext.vocab import GloVe\n","\n","import re\n","import os\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, tqdm_notebook\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Uaf4je3hKHal"},"source":["torch.manual_seed(777)\n","random.seed(777)\n","np.random.seed(777)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPt8BfAD-bqQ"},"source":["## Data setting"]},{"cell_type":"code","metadata":{"id":"RzAuE3BaaAli"},"source":["from konlpy.tag import Okt\n","okt = Okt()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8XG8uxBHjpg","executionInfo":{"status":"ok","timestamp":1636110421614,"user_tz":-540,"elapsed":133608,"user":{"displayName":"Hohyun Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18073395245965959785"}},"outputId":"91e535d2-200a-4b96-f340-2834a7379ab1"},"source":["torch.manual_seed(777)\n","random.seed(777)\n","np.random.seed(777)\n","\n","train = pd.read_csv(\"/content/drive/My Drive/crawling_20211104.csv\")\n","df_shuffled=train.iloc[np.random.permutation(train.index)].reset_index(drop=True)\n","train = df_shuffled.copy()\n","\n","label = []\n","for s in train['scores']:\n","  if s == \"width: 100%\":\n","    label.append(0)\n","  elif s == \"width: 80%\":\n","    label.append(1)\n","  else:\n","    label.append(2)\n","\n","train['label'] = label\n","\n","train_df = train[:-8000]\n","val_df = train[-8000:]\n","\n","total_tokens = [okt.morphs(sentence) for sentence in tqdm(train['rvs'])]\n","\n","stopwords = ['의','가','이','은','들','는','과','도','를','으로','자','에','와','한','하다','대다','년','월','대']"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30000/30000 [02:12<00:00, 226.40it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"WxSky7dsGZfP"},"source":["using_data = train[['rvs', 'label', 'retypes', 'categories', 'meta_sizes', 'meta_brights', 'meta_colors', 'meta_thicks']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTQCu0SGFuCK"},"source":["meta_df = pd.DataFrame()\n","for c in using_data.columns.tolist()[2:]:\n","  dummy_cate = pd.get_dummies(using_data[c])\n","  meta_df = pd.concat([meta_df, dummy_cate], axis=1)\n","\n","train_meta_df = meta_df[:-8000]\n","val_meta_df = meta_df[-8000:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6gvAi0wNHjph"},"source":["### 단어 집합 만들기 with GloVe (torchtext)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAc4TYrAHjph","executionInfo":{"status":"ok","timestamp":1636110650505,"user_tz":-540,"elapsed":228925,"user":{"displayName":"Hohyun Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18073395245965959785"}},"outputId":"433f567d-c3e2-4ce5-f6ed-56887470a130"},"source":["# 필드 정의\n","TEXT = data.Field(sequential=True,\n","                  use_vocab=True,\n","                  tokenize=okt.morphs,\n","                  lower=True,\n","                  batch_first=True,\n","                  stop_words = stopwords,\n","                  fix_length=100)\n","\n","LABEL = data.LabelField(sequential=False,\n","                   use_vocab=False,\n","                   batch_first=False,\n","                   is_target=True)\n","\n","TEXT.build_vocab(total_tokens,  vectors=GloVe(name='6B', dim=200), min_freq=3, max_size=10000)\n","LABEL.build_vocab(['neg','pos'])\n","\n","vocab = TEXT.vocab\n","print('단어 집합의 크기 : {}'.format(len(vocab)))\n","vocab_size = len(vocab)\n","\n","word_dict = TEXT.vocab.stoi\n","rev_word_dict = {v:k for k, v in word_dict.items()}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:45, 5.21MB/s]                           \n","100%|█████████▉| 399999/400000 [00:32<00:00, 12405.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["단어 집합의 크기 : 8949\n"]}]},{"cell_type":"code","metadata":{"id":"ThCcwukIBAuq"},"source":["using_train_data = train_df[['rvs', 'label', 'categories', 'retypes',\n","       'meta_sizes', 'meta_brights', 'meta_colors', 'meta_thicks', ]]\n","      #  'pur_option', 'cus_sex', 'cus_height', 'cus_weight']]\n","\n","using_val_data = val_df[['rvs', 'label', 'categories', 'retypes',\n","       'meta_sizes', 'meta_brights', 'meta_colors', 'meta_thicks', ]]\n","      #  'pur_option', 'cus_sex', 'cus_height', 'cus_weight']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJNlRoATKAoh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636110650509,"user_tz":-540,"elapsed":44,"user":{"displayName":"Hohyun Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18073395245965959785"}},"outputId":"d6d7d539-49ea-43b5-8fde-9c831c0bb207"},"source":[" class BinaryDataset(Dataset):\n","\n","    def __init__(self, df, meta_df, word_dict, min_len, only_ct = True, test=False):\n","        self.df = df\n","        self.meta_df = meta_df\n","        self.w2i = word_dict\n","        self.i2w = {v:k for k, v in word_dict.items()}\n","        self.min_len = min_len\n","        self.test = test\n","        self.only_ct = only_ct\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        sentence = self.df.iloc[idx, 0]\n","        label = self.df.iloc[idx, 1]\n","\n","        if self.only_ct:\n","          meta = self.meta_df.iloc[idx, :]\n","        else:\n","          meta = self.meta_df.iloc[idx, 3:9]\n","\n","        text = [tok for tok in word_tokenize(sentence.lower())]\n","        if len(text) < self.min_len:\n","            text += [TEXT.pad_token] * (self.min_len - len(text))\n","        else :\n","            text = text[:self.min_len]\n","        indexed = torch.tensor([self.w2i[t] for t in text])\n","\n","        return indexed, torch.tensor(label), torch.tensor(meta).float()\n","\n","BATCH = 16\n","\n","train_data = BinaryDataset(using_train_data, train_meta_df, word_dict, 64)\n","train_loader = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n","\n","val_data = BinaryDataset(using_val_data, val_meta_df, word_dict, 64)\n","eval_loader = DataLoader(val_data, batch_size=BATCH, shuffle=False)\n","\n","print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n","print('검증 샘플의 개수 : {}'.format(len(val_data)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 샘플의 개수 : 22000\n","검증 샘플의 개수 : 8000\n"]}]},{"cell_type":"code","metadata":{"id":"AysTakqARF1R"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device_type = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HriN4dFd6LVh"},"source":["class ConstantsClass():\n","    def __init__(self):\n","        self.UNK = 0\n","        self.PAD = 1\n","        self.BOS = 2\n","        self.EOS = 3\n","        self.PAD_WORD = '<pad>'\n","        self.UNK_WORD = '<unk>'\n","        self.BOS_WORD = '<s>'\n","        self.EOS_WORD = '</s>'\n","\n","\n","Constants = ConstantsClass()\n","\n","\n","class Linear(nn.Module):\n","    ''' Simple Linear layer with xavier init '''\n","\n","    def __init__(self, d_in, d_out, bias=True):\n","        super(Linear, self).__init__()\n","        self.linear = nn.Linear(d_in, d_out, bias=bias)\n","        torch.nn.init.xavier_normal(self.linear.weight)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","\n","class Bottle(nn.Module):\n","    ''' Perform the reshape routine before and after an operation '''\n","\n","    def forward(self, input):\n","        if len(input.size()) <= 2:\n","            return super(Bottle, self).forward(input)\n","        size = input.size()[:2]\n","        out = super(Bottle, self).forward(input.view(size[0] * size[1], -1))\n","        return out.view(size[0], size[1], -1)\n","\n","\n","class BottleLinear(Bottle, Linear):\n","    ''' Perform the reshape routine before and after a linear projection '''\n","    pass\n","\n","\n","class BottleSoftmax(Bottle, nn.Softmax):\n","    ''' Perform the reshape routine before and after a softmax operation'''\n","    pass\n","\n","\n","class LayerNormalization(nn.Module):\n","    ''' Layer normalization module '''\n","\n","    def __init__(self, d_hid, eps=1e-3):\n","        super(LayerNormalization, self).__init__()\n","\n","        self.eps = eps\n","        self.a_2 = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n","        self.b_2 = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n","\n","    def forward(self, z):\n","        if z.size(1) == 1:\n","            return z\n","\n","        mu = torch.mean(z, keepdim=True, dim=-1)\n","        sigma = torch.std(z, keepdim=True, dim=-1)\n","        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n","        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n","\n","        return ln_out\n","\n","\n","class BatchBottle(nn.Module):\n","    ''' Perform the reshape routine before and after an operation '''\n","\n","    def forward(self, input):\n","        if len(input.size()) <= 2:\n","            return super(BatchBottle, self).forward(input)\n","        size = input.size()[1:]\n","        out = super(BatchBottle, self).forward(input.view(-1, size[0] * size[1]))\n","        return out.view(-1, size[0], size[1])\n","\n","\n","class BottleLayerNormalization(BatchBottle, LayerNormalization):\n","    ''' Perform the reshape routine before and after a layer normalization'''\n","    pass\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    ''' Scaled Dot-Product Attention '''\n","\n","    def __init__(self, d_model, attn_dropout=0.1):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.temper = np.power(d_model, 0.5)\n","        self.dropout = nn.Dropout(attn_dropout)\n","        self.softmax = BottleSoftmax()\n","\n","    def forward(self, q, k, v, attn_mask=None):\n","        attn = torch.bmm(q, k.transpose(1, 2)) / self.temper\n","\n","        if attn_mask is not None:\n","            assert attn_mask.size() == attn.size(), \\\n","                'Attention mask shape {} mismatch ' \\\n","                'with Attention logit tensor shape ' \\\n","                '{}.'.format(attn_mask.size(), attn.size())\n","\n","            attn.data.masked_fill_(attn_mask, -float('inf'))\n","\n","        attn = self.softmax(attn)\n","        attn = self.dropout(attn)\n","        output = torch.bmm(attn, v)\n","\n","        return output, attn\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    ''' Multi-Head Attention module '''\n","\n","    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n","        super(MultiHeadAttention, self).__init__()\n","\n","        self.n_head = n_head\n","        self.d_k = d_k\n","        self.d_v = d_v\n","\n","        self.w_qs = nn.Parameter(torch.FloatTensor(n_head, d_model, d_k))\n","        self.w_ks = nn.Parameter(torch.FloatTensor(n_head, d_model, d_k))\n","        self.w_vs = nn.Parameter(torch.FloatTensor(n_head, d_model, d_v))\n","\n","        self.attention = ScaledDotProductAttention(d_model)\n","        self.layer_norm = LayerNormalization(d_model)\n","        self.proj = Linear(n_head * d_v, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        torch.nn.init.xavier_normal(self.w_qs)\n","        torch.nn.init.xavier_normal(self.w_ks)\n","        torch.nn.init.xavier_normal(self.w_vs)\n","\n","    def forward(self, q, k, v, attn_mask=None):\n","        d_k, d_v = self.d_k, self.d_v\n","        n_head = self.n_head\n","\n","        residual = q\n","\n","        mb_size, len_q, d_model = q.size()\n","        mb_size, len_k, d_model = k.size()\n","        mb_size, len_v, d_model = v.size()\n","\n","        # treat as a (n_head) size batch\n","        q_s = q.repeat(n_head, 1, 1).view(n_head, -1, d_model)  # n_head x (mb_size*len_q) x d_model\n","        k_s = k.repeat(n_head, 1, 1).view(n_head, -1, d_model)  # n_head x (mb_size*len_k) x d_model\n","        v_s = v.repeat(n_head, 1, 1).view(n_head, -1, d_model)  # n_head x (mb_size*len_v) x d_model\n","\n","        # treat the result as a (n_head * mb_size) size batch\n","        q_s = torch.bmm(q_s, self.w_qs).view(-1, len_q, d_k)  # (n_head*mb_size) x len_q x d_k\n","        k_s = torch.bmm(k_s, self.w_ks).view(-1, len_k, d_k)  # (n_head*mb_size) x len_k x d_k\n","        v_s = torch.bmm(v_s, self.w_vs).view(-1, len_v, d_v)  # (n_head*mb_size) x len_v x d_v\n","\n","        # perform attention, result size = (n_head * mb_size) x len_q x d_v\n","        outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n","\n","        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)\n","        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)\n","\n","        # project back to residual size\n","        outputs = self.proj(outputs)\n","        outputs = self.dropout(outputs)\n","\n","        return self.layer_norm(outputs + residual), attns\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    ''' A two-feed-forward-layer module '''\n","\n","    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)  # position-wise\n","        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)  # position-wise\n","        self.layer_norm = LayerNormalization(d_hid)\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        residual = x\n","        output = self.relu(self.w_1(x.transpose(1, 2)))\n","        output = self.w_2(output).transpose(2, 1)\n","        output = self.dropout(output)\n","        return self.layer_norm(output + residual)\n","\n","\n","class EncoderLayer(nn.Module):\n","    ''' Compose with two layers '''\n","\n","    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n","        super(EncoderLayer, self).__init__()\n","        self.slf_attn = MultiHeadAttention(\n","            n_head, d_model, d_k, d_v, dropout=dropout)\n","        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n","\n","    def forward(self, enc_input, slf_attn_mask=None):\n","        enc_output, enc_slf_attn = self.slf_attn(\n","            enc_input, enc_input, enc_input, attn_mask=slf_attn_mask)\n","        enc_output = self.pos_ffn(enc_output)\n","        return enc_output, enc_slf_attn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo3fT_0-6MoR"},"source":["def position_encoding_init(n_position, d_pos_vec):\n","    ''' Init the sinusoid position encoding table '''\n","\n","    # keep dim 0 for padding token position encoding zero vector\n","    position_enc = np.array([\n","        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n","        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n","\n","    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n","    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n","    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n","\n","\n","def get_attn_padding_mask(seq_q, seq_k):\n","    ''' Indicate the padding-related part to mask '''\n","    assert seq_q.dim() == 2 and seq_k.dim() == 2\n","    mb_size, len_q = seq_q.size()\n","    mb_size, len_k = seq_k.size()\n","    pad_attn_mask = seq_k.data.eq(Constants.PAD).unsqueeze(1)  # bx1xsk\n","    pad_attn_mask = pad_attn_mask.expand(mb_size, len_q, len_k)  # bxsqxsk\n","    return pad_attn_mask\n","\n","\n","def get_attn_subsequent_mask(seq):\n","    ''' Get an attention mask to avoid using the subsequent info.'''\n","    assert seq.dim() == 2\n","    attn_shape = (seq.size(0), seq.size(1), seq.size(1))\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    subsequent_mask = torch.from_numpy(subsequent_mask)\n","    if seq.is_cuda:\n","        subsequent_mask = subsequent_mask.cuda()\n","    return subsequent_mask\n","\n","\n","class Encoder(nn.Module):\n","    ''' A encoder model with self attention mechanism. '''\n","\n","    def __init__(\n","            self, n_src_vocab, n_max_seq, n_layers=6, n_head=8, d_k=64, d_v=64,\n","            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n","\n","        super(Encoder, self).__init__()\n","\n","        n_position = n_max_seq + 1\n","        self.n_max_seq = n_max_seq\n","        self.d_model = d_model\n","\n","        self.position_enc = nn.Embedding(n_position, d_word_vec, padding_idx=Constants.PAD)\n","        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n","\n","        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=Constants.PAD)\n","\n","        self.layer_stack = nn.ModuleList([\n","            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n","            for _ in range(n_layers)])\n","\n","    def forward(self, src_seq, src_pos, return_attns=False):\n","        # Word embedding look up\n","        enc_input = self.src_word_emb(src_seq)\n","\n","        # Position Encoding addition\n","        enc_input += self.position_enc(src_pos)\n","        if return_attns:\n","            enc_slf_attns = []\n","\n","        enc_output = enc_input\n","        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n","        for enc_layer in self.layer_stack:\n","            enc_output, enc_slf_attn = enc_layer(\n","                enc_output, slf_attn_mask=enc_slf_attn_mask)\n","            if return_attns:\n","                enc_slf_attns += [enc_slf_attn]\n","\n","        if return_attns:\n","            return enc_output, enc_slf_attns\n","        else:\n","            return enc_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLeDr9qd6PYT"},"source":["class MultiHeadAttention_classifier(nn.Module):\n","    def __init__(self, vocab, meta_input_dim, meta_hidden_dim, n_layers=6, n_head=8, max_seq_len = 64, label_size = 3, use_m = True,\n","                 d_word_vec=128, d_model=128, d_inner_hid=256, d_k=32, d_v=32,\n","                 dropout=0.1, proj_share_weight=True, embs_share_weight=True):\n","        super(MultiHeadAttention_classifier, self).__init__()\n","        vocab_size = len(vocab)\n","        max_seq_len = max_seq_len\n","        label_size = label_size\n","        self.use_m = use_m\n","\n","        self.encoder = Encoder(\n","            vocab_size, max_seq_len, n_layers=n_layers, n_head=n_head,\n","            d_word_vec=d_word_vec, d_model=d_model,\n","            d_inner_hid=d_inner_hid, dropout=dropout)\n","        \n","        self.meta_feature = nn.Sequential(\n","            nn.Linear(meta_input_dim, meta_hidden_dim),\n","            nn.Linear(meta_hidden_dim, meta_input_dim)\n","        )\n","        if self.use_m :\n","          self.hidden2label = nn.Sequential(\n","              nn.Linear(max_seq_len * d_model + meta_input_dim, d_model),\n","              nn.Linear(d_model, label_size)\n","          )\n","\n","        else:\n","          self.hidden2label = nn.Sequential(\n","              nn.Linear(max_seq_len * d_model, d_model),\n","              nn.Linear(d_model, label_size)\n","          )\n","\n","    def forward(self, seq, meta, lengths=None):\n","        batch_size = seq.size(0)\n","        pos = np.array([[pos_i + 1 if w_i != Constants.PAD else 0 for pos_i, w_i in enumerate(inst)] for inst in seq])\n","        pos = torch.from_numpy(pos)\n","        if torch.cuda.is_available():\n","            pos = pos.cuda()\n","        enc_output = self.encoder(seq, pos)\n","\n","        if self.use_m :\n","          # meta = meta.unsqueeze(1)\n","          meta_f = self.meta_feature(meta)\n","          cat = torch.cat([enc_output.reshape((batch_size, -1)), meta_f], dim=1)\n","          out = self.hidden2label(cat)\n","        else:\n","          out = self.hidden2label(enc_output.reshape((batch_size, -1)))\n","        \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C39f_8mP7bxn","executionInfo":{"status":"ok","timestamp":1636111879938,"user_tz":-540,"elapsed":54,"user":{"displayName":"Hohyun Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18073395245965959785"}},"outputId":"85954288-60a3-4664-df1a-1148116899d1"},"source":["META_INPUT = 21\n","META_HIDDEN = 64\n","\n","model = MultiHeadAttention_classifier(vocab, META_INPUT, META_INPUT)\n","optimizer = optim.Adam(model.parameters())\n","\n","criterion = nn.CrossEntropyLoss()\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:131: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:132: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:133: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"]}]},{"cell_type":"code","metadata":{"id":"oQukY7AtRJ4V"},"source":["def binary_accuracy(preds, y):\n","\n","    output_softmax = torch.log_softmax(preds, 1)\n","    _, output_tags = torch.max(output_softmax, 1)\n","    correct_pred = (output_tags == y).float()\n","    acc = correct_pred.sum() / len(correct_pred)\n","    # acc_sum += acc * len(correct_pred)\n","    return acc, len(correct_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVD4okweRLzt"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    acc_sum = 0\n","\n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        text, label, meta = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n","        outputs = model(text, meta)\n","        predictions = F.softmax(outputs, dim=1)\n","        loss = criterion(predictions, label)\n","        \n","        acc, leng = binary_accuracy(outputs, label)\n","        acc_sum += acc * leng\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7cx06BVb9-e"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text, label, meta = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n","\n","            outputs = model(text, meta)#.squeeze(1)\n","            predictions = F.softmax(outputs, dim=1)\n","\n","            loss = criterion(predictions, label)\n","            \n","            acc, _ = binary_accuracy(outputs, label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKfjKP1VcFvW"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfTAk2HP_3Uc"},"source":["from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n","from sklearn.metrics import recall_score, f1_score, roc_auc_score\n","\n","def get_clf_eval(y_test=None, pred=None):\n","    confusion = confusion_matrix(y_test, pred)\n","    accuracy = accuracy_score(y_test, pred)\n","    precision = precision_score(y_test, pred)\n","    recall = recall_score(y_test, pred)\n","    f1 = f1_score(y_test, pred)\n","\n","    roc_auc = roc_auc_score(y_test, pred)\n","    print(\"Confusion Matrix\")\n","    print(\"*\"*20)\n","    print(confusion)\n","    print(\"*\"*20)\n","    print(\"ACC: {0:.4f}, Precision : {1: .4f}, Recall : {2: .4f}, F1_score : {3:.4f}, AUC : {4:.4f}\"\n","    .format(accuracy, precision, recall,f1,roc_auc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yea4ANK0SDkl"},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, eval_loader, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'lstm_model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rWVfplS_O_C"},"source":[""],"execution_count":null,"outputs":[]}]}